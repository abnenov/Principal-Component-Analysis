# Анализ на главните компоненти (PCA)

## 1. Какво са собствени стойности и собствени вектори?

**Собствен вектор** на квадратна матрица A е ненулев вектор v, който при умножение с матрицата A се преобразува в същия вектор, умножен по скаларна стойност λ:

A·v = λ·v

**Собствена стойност** е скаларът λ, който показва с колко се "разтяга" или "свива" собственият вектор при умножение с матрицата А.

Геометрично това означава, че при прилагане на линейната трансформация, представена от матрицата A, собственият вектор променя само размера си, но не и посоката.

## 2. Какво е собствен базис? Какво е спектър на матрица?

**Собствен базис** е множество от линейно независими собствени вектори на матрица. Ако матрицата е n×n, максималният брой линейно независими собствени вектори е n. Когато матрицата има n линейно независими собствени вектори, тя може да бъде диагонализирана.

**Спектър на матрица** е множеството от всички собствени стойности на матрицата. Спектърът дава важна информация за свойствата на матрицата като:
- Дали матрицата е обратима (няма нулеви собствени стойности)
- Дали матрицата е положително дефинитна (всички собствени стойности са положителни)
- Условното число на матрицата (отношението между най-голямата и най-малката собствена стойност)

## 3. Как изчисляваме собствените стойности и вектори на матрица?

За матрица A собствените стойности λ са решенията на характеристичното уравнение:

det(A - λI) = 0

където I е единичната матрица.

След намиране на собствените стойности, за всяка собствена стойност λ решаваме системата:

(A - λI)·v = 0

за да намерим съответния собствен вектор v.

На практика, в численото програмиране се използват итеративни методи като степенния метод, QR алгоритъм или метода на Якоби за изчисляване на собствените стойности и собствените вектори.

## 4. Какво е проекция?

**Проекция** е линейна трансформация, която трансформира вектори от едно пространство в друго пространство (обикновено с по-малка размерност). 

Математически, проекцията на вектор v върху направление, определено от единичен вектор u, се дава от формулата:

proj_u(v) = (v·u)·u

където v·u е скаларното произведение на v и u.

В контекста на PCA, проекцията е операцията, която преобразува данните от първоначалното пространство в новото пространство, определено от главните компоненти.

## 5. Как проекцията запазва някои форми?

Когато проектираме обект, някои характеристики на формата се запазват, а други се губят.

Аналогия със сянка: Когато гледаме сянката на обект, ние виждаме проекция на 3D обект върху 2D повърхност. В зависимост от ъгъла на светлината, сянката може да запази важни характеристики на обекта:

- При оптимален ъгъл, сянката може да разкрие силуета или важни контури
- При неоптимален ъгъл, сянката може да скрие или деформира важни характеристики

В PCA, алгоритъмът избира такива направления за проекция, че максимално количество от вариацията (информацията) в данните се запазва. Това е аналогично на избор на най-добрия ъгъл за проектиране на сянка, така че тя да показва максимално много информация за обекта.

## 6. Каква е връзката между проекцията и собствените стойности/вектори?

В PCA, направленията, по които проектираме данните, са именно собствените вектори на ковариационната матрица на данните.

Собствените вектори определят "осите" на новата координатна система, а съответните собствени стойности показват колко от общата вариация на данните се "улавя" по всяка ос.

Собственият вектор с най-голяма собствена стойност определя направлението, по което данните се разпростират най-много (имат най-голяма вариация). Вторият собствен вектор е перпендикулярен на първия и показва второто най-важно направление на вариация, и т.н.

## 7. Какво е PCA?

**Principal Component Analysis (PCA)** е техника за намаляване на размерността, която трансформира набор от възможно корелирани променливи в по-малък набор от линейно некорелирани променливи, наречени главни компоненти.

Основните цели на PCA са:
1. Намаляване на размерността на данните
2. Запазване на максимално количество информация (вариация) от оригиналните данни
3. Премахване на шума и излишната информация
4. Визуализация на многомерни данни

PCA осъществява линейна трансформация на данните, избирайки нови оси (главни компоненти), които са перпендикулярни една на друга и подредени по важност.

## 8. Какво са главните компоненти? Колко компонента има?

**Главните компоненти** са линейни комбинации на оригиналните променливи, подредени по намаляваща вариация.

- Първият главен компонент улавя максималната вариация в данните
- Вторият главен компонент е перпендикулярен на първия и улавя максималната остатъчна вариация
- И така нататък за всеки следващ компонент

Броят на възможните главни компоненти е равен на броя на оригиналните променливи (размерността на данните). Ако имаме данни с d променливи, то максималният брой главни компоненти е d.

На практика, обаче, често се избират само първите k компонента (k < d), които обясняват достатъчно голям процент от общата вариация, например 90% или 95%.

## 9. Какво е дисперсия? Какво е обяснена дисперсия?

**Дисперсия (вариация)** е статистическа мярка, която показва колко разпръснати са данните около средната им стойност. Математически, дисперсията на набор от стойности е средната стойност на квадратите на отклоненията от средната аритметична стойност.

**Обяснена дисперсия** е частта от общата дисперсия в данните, която може да бъде "обяснена" или "улавена" от определена променлива или компонент. В контекста на PCA, обяснената дисперсия за всеки главен компонент показва колко от общата вариация в данните се улавя от този компонент.

## 10. Как главните компоненти се свързват с обяснената дисперсия?

Собствената стойност, съответстваща на всеки главен компонент, представлява количеството на вариацията, която този компонент обяснява.

Ако λ_i е собствената стойност на i-тия главен компонент, то частта от общата дисперсия, обяснена от този компонент, е:

λ_i / Σλ_j  (сумата е за всички j)

Обикновено главните компоненти се подреждат в низходящ ред на техните собствени стойности, така че:
- Първият главен компонент (PC1) обяснява най-голям процент от вариацията
- Вторият (PC2) обяснява втория най-голям процент
- И т.н.

Кумулативната обяснена дисперсия показва колко от общата вариация се улавя от първите k компонента взети заедно.

## 11. Как се имплементира PCA? Имплементация

Ето алгоритъм за PCA:

```python
import numpy as np

def my_pca(X, n_components):
    """
    Имплементация на PCA алгоритъм
    
    Параметри:
    X : numpy array, форма (n_samples, n_features)
        Входящите данни
    n_components : int
        Брой главни компоненти, които да се върнат
        
    Връща:
    X_pca : numpy array, форма (n_samples, n_components)
        Проектираните данни
    components : numpy array, форма (n_components, n_features)
        Главните компоненти (собствените вектори)
    explained_variance : numpy array, форма (n_components,)
        Обяснената вариация за всеки компонент
    """
    # Стъпка 1: Стандартизиране на данните
    X_centered = X - np.mean(X, axis=0)
    
    # Стъпка 2: Изчисляване на ковариационната матрица
    cov_matrix = np.cov(X_centered, rowvar=False)
    
    # Стъпка 3: Изчисляване на собствените стойности и вектори
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    
    # Стъпка 4: Сортиране на собствените стойности и вектори в низходящ ред
    idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[idx]
    eigenvectors = eigenvectors[:, idx]
    
    # Стъпка 5: Избиране на top n_components
    components = eigenvectors[:, :n_components]
    explained_variance = eigenvalues[:n_components]
    
    # Стъпка 6: Проектиране на данните върху главните компоненти
    X_pca = X_centered.dot(components)
    
    return X_pca, components, explained_variance
```

## 12. Приложения на PCA: редуциране на 3D изображение до 2D

Нека демонстрираме как PCA може да се използва за редуциране на размерността на данни от 3D до 2D:

```python
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.datasets import make_blobs

# Генериране на 3D данни
np.random.seed(42)
X, y = make_blobs(n_samples=300, centers=4, n_features=3, random_state=42)

# Прилагане на PCA
X_pca, components, explained_variance = my_pca(X, n_components=2)

# Изчисляване на обяснена вариация в проценти
total_variance = np.sum(np.var(X, axis=0))
explained_variance_ratio = explained_variance / total_variance

# Визуализация на 3D данни и 2D проекция
fig = plt.figure(figsize=(12, 5))

# Оригинални 3D данни
ax1 = fig.add_subplot(121, projection='3d')
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=y, cmap='viridis', s=30)
ax1.set_title('Оригинални 3D данни')
ax1.set_xlabel('Характеристика 1')
ax1.set_ylabel('Характеристика 2')
ax1.set_zlabel('Характеристика 3')

# 2D проекция след PCA
ax2 = fig.add_subplot(122)
ax2.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', s=30)
ax2.set_title('2D проекция след PCA')
ax2.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
ax2.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
ax2.grid(True)

plt.tight_layout()
plt.show()
```

Този код ще показва оригиналните 3D данни и тяхната 2D проекция, получена чрез PCA. Главните компоненти са избрани така, че да максимизират запазената вариация.

## 13. Практическо приложение на PCA: визуализация на многомерни данни

Нека демонстрираме как PCA може да се използва за визуализация на данни с висока размерност:

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler

# Зареждане на Iris dataset (4D данни)
iris = load_iris()
X_iris = iris.data
y_iris = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

# Стандартизиране на данните
scaler = StandardScaler()
X_iris_scaled = scaler.fit_transform(X_iris)

# Прилагане на PCA
X_pca, components, explained_variance = my_pca(X_iris_scaled, n_components=3)

# Изчисляване на обяснена вариация
total_variance = np.sum(np.var(X_iris_scaled, axis=0))
explained_variance_ratio = explained_variance / total_variance

# 3D визуализация
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

colors = ['navy', 'turquoise', 'darkorange']

for color, i, target_name in zip(colors, [0, 1, 2], target_names):
    ax.scatter(X_pca[y_iris == i, 0], X_pca[y_iris == i, 1], X_pca[y_iris == i, 2],
               color=color, alpha=0.8, label=target_name)
    
ax.set_title('PCA на Iris dataset (4D -> 3D)')
ax.set_xlabel(f'PC1 ({explained_variance_ratio[0]*100:.1f}%)')
ax.set_ylabel(f'PC2 ({explained_variance_ratio[1]*100:.1f}%)')
ax.set_zlabel(f'PC3 ({explained_variance_ratio[2]*100:.1f}%)')
ax.legend()

plt.tight_layout()
plt.show()

# Анализ на главните компоненти
plt.figure(figsize=(12, 6))
# Тегла на компонентите
plt.imshow(components.T, cmap='viridis')
plt.yticks(range(len(feature_names)), feature_names)
plt.xticks([0, 1, 2], [f'PC1 ({explained_variance_ratio[0]*100:.1f}%)',
                       f'PC2 ({explained_variance_ratio[1]*100:.1f}%)',
                       f'PC3 ({explained_variance_ratio[2]*100:.1f}%)'])
plt.colorbar(label='Тегло на компонента')
plt.title('Тегла на главните компоненти за Iris dataset')
plt.tight_layout()
plt.show()
```

Този код визуализира Iris dataset (който има 4 измерения) в 3D пространство, чрез използване на PCA. Той също така показва тежестите на всяка оригинална променлива във всеки главен компонент.

## Заключение

PCA е мощен метод за намаляване на размерността, който позволява:
- Визуализация на многомерни данни
- Намаляване на изчислителната сложност чрез намаляване на броя на характеристиките
- Премахване на шума и излишната информация
- Извличане на най-важните характеристики от данните

PCA се използва в множество области като:
- Разпознаване на образи и компютърно зрение
- Компресиране на данни
- Анализ на данни от експерименти (напр. генетични експерименти)
- Дименсионално редуциране за алгоритми за машинно обучение
